{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe6acf56-a4ba-4fb8-99ce-388b03846413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 1: 100%|█████████████████████| 128/128 [00:09<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 2: 100%|█████████████████████| 128/128 [00:09<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 3: 100%|█████████████████████| 128/128 [00:09<00:00, 13.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 4: 100%|█████████████████████| 128/128 [00:09<00:00, 13.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 5: 100%|█████████████████████| 128/128 [00:09<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 6: 100%|█████████████████████| 128/128 [00:09<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 7: 100%|█████████████████████| 128/128 [00:09<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 8: 100%|█████████████████████| 128/128 [00:09<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 9: 100%|█████████████████████| 128/128 [00:09<00:00, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 10: 100%|████████████████████| 128/128 [00:09<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 11: 100%|████████████████████| 128/128 [00:09<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 12: 100%|████████████████████| 128/128 [00:09<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 13: 100%|████████████████████| 128/128 [00:10<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 14: 100%|████████████████████| 128/128 [00:10<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 15: 100%|████████████████████| 128/128 [00:10<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 16: 100%|████████████████████| 128/128 [00:10<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 17: 100%|████████████████████| 128/128 [00:10<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 18: 100%|████████████████████| 128/128 [00:10<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 19: 100%|████████████████████| 128/128 [00:10<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 20: 100%|██████████████████████| 26/26 [00:02<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 saved to ../../Data/TF_Embeddings_Multiclass/tf_embeddings_batch_20.csv\n",
      "All batches have been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "edges = pd.read_table(\"../../Data/tables/T2.regulome.edges.txt\")\n",
    "nodes = pd.read_table(\"../../Data/tables/T3.regulome.nodes.txt\")\n",
    "nodes['chrm'] = nodes['chr']\n",
    "\n",
    "# Load the precomputed downstream nodes from file\n",
    "def load_precomputed_downstream_nodes(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        downstream_map = pickle.load(f)\n",
    "    return downstream_map\n",
    "\n",
    "# Load precomputed downstream nodes\n",
    "downstream_map = load_precomputed_downstream_nodes('../../Data/Precomputed_Downstream_Nodes.pkl')\n",
    "\n",
    "# Define the GRN map and node list\n",
    "all_nodes = nodes['name'].tolist()\n",
    "N = len(all_nodes)\n",
    "node_to_index = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "\n",
    "# Function to process a batch of files and generate multiclass embeddings\n",
    "def process_batch(batch_files, batch_number, tf_match_results_dir, output_dir):\n",
    "    tf_embeddings = {}  # Initialize embeddings dictionary for this batch\n",
    "\n",
    "    with tqdm(total=len(batch_files), desc=f\"Processing Batch {batch_number}\") as pbar:\n",
    "        for file in batch_files:\n",
    "            individual_id = file.split('_')[2]  # Assuming file names have individual IDs\n",
    "            individual_vector = np.zeros(N, dtype=int)  # Default value 0\n",
    "\n",
    "            # Load the matching results for this individual\n",
    "            file_path = os.path.join(tf_match_results_dir, file)\n",
    "            df = pd.read_csv(file_path, sep='\\t', header=None, names=['TF'], usecols=[8])\n",
    "\n",
    "            # Set direct mutations (TF nodes) to 2\n",
    "            for tf_node in df['TF'].unique():\n",
    "                if tf_node in downstream_map:\n",
    "                    tf_index = node_to_index.get(tf_node)\n",
    "                    \n",
    "                    # Set downstream nodes to 1\n",
    "                    downstream_indices = [node_to_index[node] for node in downstream_map[tf_node] if node in node_to_index]\n",
    "                    individual_vector[downstream_indices] = 1  # Downstream, set to 1\n",
    "                    if tf_index is not None:\n",
    "                        individual_vector[tf_index] = 2  # Direct match, set to 2\n",
    "\n",
    "            # Store the embedding for this individual\n",
    "            tf_embeddings[individual_id] = individual_vector\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Convert the batch embeddings to a DataFrame and save the results\n",
    "    tf_embeddings_df = pd.DataFrame.from_dict(tf_embeddings, orient='index', columns=all_nodes)\n",
    "    batch_output_file = os.path.join(output_dir, f'tf_embeddings_batch_{batch_number}.csv')\n",
    "    tf_embeddings_df.to_csv(batch_output_file)\n",
    "\n",
    "    print(f\"Batch {batch_number} saved to {batch_output_file}\")\n",
    "\n",
    "# Load and process the TF_Match_Results folder\n",
    "tf_match_results_dir = \"../../Data/TF_Match_Results\"\n",
    "tf_files = [file for file in os.listdir(tf_match_results_dir) if file.endswith(\".bed\")]\n",
    "\n",
    "# Process the files in batches of 128\n",
    "batch_size = 128\n",
    "num_batches = len(tf_files) // batch_size + (1 if len(tf_files) % batch_size != 0 else 0)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = '../../Data/TF_Embeddings_Multiclass'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    start_index = batch_num * batch_size\n",
    "    end_index = min(start_index + batch_size, len(tf_files))\n",
    "    batch_files = tf_files[start_index:end_index]\n",
    "\n",
    "    # Process the current batch of files and save after each batch\n",
    "    process_batch(batch_files, batch_num + 1, tf_match_results_dir, output_dir)\n",
    "\n",
    "print(\"All batches have been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a774e33-5ee7-44db-8734-6af6178ac909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../Data/TF_Match_Results/enhancer_matched_SSC08007.bed'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e57a36f-4d3e-4d80-a1b3-e04c351c5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'tf_matched_SSC08007.bed'\n",
    "individual_id = file.split('_')[2]  # Assuming file names have individual IDs\n",
    "individual_vector = np.zeros(N, dtype=int)  # Default value 0\n",
    "\n",
    "# Load the matching results for this individual\n",
    "file_path = os.path.join(tf_match_results_dir, file)\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['TF'], usecols=[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8de087a-b1c8-441a-9e0f-5afff30b7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set direct mutations (TF nodes) to 2\n",
    "for tf_node in df['TF'].unique():\n",
    "    if tf_node in downstream_map:\n",
    "        tf_index = node_to_index.get(tf_node)\n",
    "        \n",
    "        # Set downstream nodes to 1\n",
    "        downstream_indices = [node_to_index[node] for node in downstream_map[tf_node] if node in node_to_index]\n",
    "        individual_vector[downstream_indices] = 1  # Downstream, set to 1\n",
    "        if tf_index is not None:\n",
    "            individual_vector[tf_index] = 2  # Direct match, set to 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d0ae753-667a-4c2b-995a-d7267441704c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(individual_vector == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e78374-2e76-430c-8d43-40714b408dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 1: 100%|███████████| 128/128 [00:00<00:00, 135.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 2: 100%|███████████| 128/128 [00:00<00:00, 186.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 3: 100%|███████████| 128/128 [00:00<00:00, 144.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 4: 100%|███████████| 128/128 [00:00<00:00, 178.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 5: 100%|███████████| 128/128 [00:00<00:00, 200.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 6: 100%|███████████| 128/128 [00:00<00:00, 156.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 7: 100%|███████████| 128/128 [00:00<00:00, 164.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 8: 100%|███████████| 128/128 [00:00<00:00, 167.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 9: 100%|███████████| 128/128 [00:00<00:00, 137.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 10: 100%|██████████| 128/128 [00:00<00:00, 149.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 11: 100%|██████████| 128/128 [00:00<00:00, 130.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 12: 100%|██████████| 128/128 [00:01<00:00, 124.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 13: 100%|██████████| 128/128 [00:00<00:00, 151.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 14: 100%|██████████| 128/128 [00:00<00:00, 165.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 15: 100%|██████████| 128/128 [00:00<00:00, 141.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 16: 100%|██████████| 128/128 [00:00<00:00, 173.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 17: 100%|██████████| 128/128 [00:00<00:00, 179.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 18: 100%|██████████| 128/128 [00:00<00:00, 175.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 19: 100%|██████████| 128/128 [00:00<00:00, 168.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Enhancer Batch 20: 100%|████████████| 26/26 [00:00<00:00, 196.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 saved to ../../Data/Enhancer_Embeddings_Multiclass/enhancer_embeddings_batch_20.csv\n",
      "All enhancer batches have been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Load the precomputed downstream nodes from file\n",
    "def load_precomputed_downstream_nodes(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        downstream_map = pickle.load(f)\n",
    "    return downstream_map\n",
    "\n",
    "# Load precomputed downstream nodes\n",
    "downstream_map = load_precomputed_downstream_nodes('../../Data/Precomputed_Downstream_Nodes.pkl')\n",
    "\n",
    "# Define the GRN map and node list\n",
    "all_nodes = nodes['name'].tolist()\n",
    "N = len(all_nodes)\n",
    "node_to_index = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "\n",
    "# Function to process a batch of files and generate multiclass embeddings for enhancer matches\n",
    "def process_enhancer_batch(batch_files, batch_number, enhancer_match_results_dir, output_dir):\n",
    "    enhancer_embeddings = {}  # Initialize embeddings dictionary for this batch\n",
    "\n",
    "    with tqdm(total=len(batch_files), desc=f\"Processing Enhancer Batch {batch_number}\") as pbar:\n",
    "        for file in batch_files:\n",
    "            individual_id = file.split('_')[2]  # Assuming file names have individual IDs\n",
    "            individual_vector = np.zeros(N, dtype=int)  # Default value is 0\n",
    "\n",
    "            # Load the matching results for this individual\n",
    "            file_path = os.path.join(enhancer_match_results_dir, file)\n",
    "            df = pd.read_csv(file_path, sep='\\t', header=None, names=['Enhancer'], usecols=[8])\n",
    "\n",
    "            # Process each enhancer node in the file\n",
    "            for enhancer_node in df['Enhancer'].unique():\n",
    "                if enhancer_node in downstream_map:\n",
    "                    # Direct match (mutation): Set to 2\n",
    "                    enhancer_index = node_to_index.get(enhancer_node)\n",
    "                    if enhancer_index is not None:\n",
    "                        individual_vector[enhancer_index] = 2\n",
    "                        print(f\"Direct match found for {enhancer_node}, setting index {enhancer_index} to 2\")\n",
    "\n",
    "                    # Downstream nodes: Set to 1, but only if not already 2\n",
    "                    downstream_indices = [node_to_index[node] for node in downstream_map[enhancer_node] if node in node_to_index]\n",
    "                    for idx in downstream_indices:\n",
    "                        if individual_vector[idx] != 2:  # Only set to 1 if not already 2\n",
    "                            individual_vector[idx] = 1\n",
    "                    print(f\"Downstream nodes for {enhancer_node}: {downstream_map[enhancer_node]} -> Setting indices {downstream_indices} to 1 (if not already 2)\")\n",
    "\n",
    "            # Store the embedding for this individual\n",
    "            enhancer_embeddings[individual_id] = individual_vector\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Convert the batch embeddings to a DataFrame and save the results\n",
    "    enhancer_embeddings_df = pd.DataFrame.from_dict(enhancer_embeddings, orient='index', columns=all_nodes)\n",
    "    batch_output_file = os.path.join(output_dir, f'enhancer_embeddings_batch_{batch_number}.csv')\n",
    "    enhancer_embeddings_df.to_csv(batch_output_file)\n",
    "\n",
    "    print(f\"Batch {batch_number} saved to {batch_output_file}\")\n",
    "\n",
    "# Load and process the Enhancer_Match_Results folder\n",
    "enhancer_match_results_dir = \"../../Data/Enhancer_Match_Results\"\n",
    "enhancer_files = [file for file in os.listdir(enhancer_match_results_dir) if file.endswith(\".bed\")]\n",
    "\n",
    "# Process the files in batches of 128\n",
    "batch_size = 128\n",
    "num_batches = len(enhancer_files) // batch_size + (1 if len(enhancer_files) % batch_size != 0 else 0)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = '../../Data/Enhancer_Embeddings_Multiclass'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop over the batches and process them\n",
    "for batch_num in range(num_batches):\n",
    "    start_index = batch_num * batch_size\n",
    "    end_index = min(start_index + batch_size, len(enhancer_files))\n",
    "    batch_files = enhancer_files[start_index:end_index]\n",
    "\n",
    "    # Process the current batch of files and save after each batch\n",
    "    process_enhancer_batch(batch_files, batch_num + 1, enhancer_match_results_dir, output_dir)\n",
    "\n",
    "print(\"All enhancer batches have been processed and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
